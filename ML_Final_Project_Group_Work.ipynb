{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kelecheeii/Loan-Status-Prediction/blob/main/ML_Final_Project_Group_Work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kelechi V. Mbibi\n",
        "\n",
        "30221021\n",
        "\n",
        "Machine Learning (ENEL 682) Project\n",
        "\n",
        "April 2024"
      ],
      "metadata": {
        "id": "wV8dYL0ygBkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**About Dataset**\n",
        "\n",
        "The loan approval dataset is a collection of financial records and associated information used to determine the eligibility of individuals or organizations for obtaining loans from a lending institution. It includes various factors such as cibil score, income, employment status, loan term, loan amount, assets value, and loan status. This dataset is commonly used in machine learning and data analysis to develop models and algorithms that predict the likelihood of loan approval based on the given features.\n"
      ],
      "metadata": {
        "id": "zcaAYHKcvVVW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMvt4a249U2e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "VNnQxzvB9k0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "upload_d = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "O8ZGz7-AmFXa",
        "outputId": "f2fa42eb-4093-4e10-dc5c-a3040ed458b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8fdeb1ba-5e4f-4feb-b739-84eddae6d4cb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8fdeb1ba-5e4f-4feb-b739-84eddae6d4cb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving loan_approval_dataset.xlsx to loan_approval_dataset.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read the data.csv file into our workbook\n",
        "data = pd.read_csv ('/content/loan_approval_dataset.csv')"
      ],
      "metadata": {
        "id": "gSkwOHmjbGGW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "05133be7-4887-4375-951d-c1516c770d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/loan_approval_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e3a9b961b894>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read the data.csv file into our workbook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'/content/loan_approval_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/loan_approval_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# next, we print the info of our columns\n",
        "data.info()"
      ],
      "metadata": {
        "id": "pqlEzDRScTcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#there is a space before each feature"
      ],
      "metadata": {
        "id": "NKn3dpptH3LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we go on to separate the dataset into features (X) and target (y) whilst dropping the 'loan_id' column because it is inconsiquential to our analysis\n",
        "X = data.iloc[:, 1:-1]\n",
        "y = data.iloc[:, -1]"
      ],
      "metadata": {
        "id": "3SBpey9cc0n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "GY6wx5G8dK2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "0ZRvTXVkd30j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for missing values in X and y\n",
        "print(\"missing values in X: \\n{}\".format(X.isnull().sum()))\n",
        "print(\"\\n\")\n",
        "print(\"missing values in y: {}\".format(y.isnull().sum()))"
      ],
      "metadata": {
        "id": "AKY8IctgfBHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of X: {}\".format(X.shape))\n",
        "print(\"Shape of y: {} \".format(y.shape))"
      ],
      "metadata": {
        "id": "t1_aXBa9lY5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing the number of approvals and rejections\n",
        "colors = ['green', 'red']\n",
        "\n",
        "sns.countplot(x = data[' loan_status'], palette=colors)\n",
        "\n",
        "for p in plt.gca().patches:\n",
        "    plt.text(p.get_x() + p.get_width() / 2.,\n",
        "             p.get_height(),\n",
        "             '{}'.format(p.get_height()),\n",
        "             ha='center',\n",
        "             va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S2Q4TUwbH6LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#segregating the loan applicants according to income level\n",
        "def income_level(value):\n",
        "    if 100000 <= value <= 1000000:\n",
        "        return \"Low\"\n",
        "    elif 1000001 <= value <= 4000000:\n",
        "        return \"Low-Middle\"\n",
        "    elif 4000001 <= value <= 7500000:\n",
        "        return \"Upper-Middle\"\n",
        "    elif 7500001 <= value <= 10000000:\n",
        "        return \"High\"\n",
        "    else:\n",
        "        return \"Very High Income\"\n",
        "\n",
        "data['Income level'] = data[' income_annum'].apply(income_level)"
      ],
      "metadata": {
        "id": "DucOHOpjIm3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "income = data['Income level'].value_counts().reset_index()\n",
        "income = income.rename(columns={'Income level':'Income Levels', 'count': \"Number of Applicants\"})\n",
        "income"
      ],
      "metadata": {
        "id": "3OgyzDiBJREV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ploting a pie chart of loan applicants according to income level\n",
        "\n",
        "plt.pie(income['Number of Applicants'], labels=income['Income Levels'], autopct=\"%1.1f%%\")\n",
        "plt.title(\"Number of Applicants in Each Income Level\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pp2MVtboIw-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, we encode categorical values values, scale numeric values and puts it into a pre-processing pipeline\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "numeric_features = ['income_annum', 'loan_amount', 'loan_term', 'cibil_score', 'no_of_dependents',\n",
        "                    'residential_assets_value', 'commercial_assets_value', 'luxury_assets_value', 'bank_asset_value']\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "categorical_features = ['education', 'self_employed']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer,  make_column_selector(dtype_exclude=\"object\")),\n",
        "        ('cat', categorical_transformer,  make_column_selector(dtype_include=\"object\"))])"
      ],
      "metadata": {
        "id": "U2QE0w01midN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we instatiate the pipeline with Logistic regression as the placeholder\n",
        "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('model', LogisticRegression(max_iter=1000))])"
      ],
      "metadata": {
        "id": "wDM4ott9o-3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split our data into 70% training and 30% testing splits\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, stratify=y, random_state=419)"
      ],
      "metadata": {
        "id": "dZI5p6qypICT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using gridsearch, we find the best model with the best parameters that gives us the best results\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = [\n",
        "    {'model': [LogisticRegression(max_iter=1000)],\n",
        "     'model__C': [0.1, 1, 10, 100],\n",
        "     'preprocessor': [preprocessor]},\n",
        "\n",
        "\n",
        "    {'model': [KNeighborsClassifier()],\n",
        "     'preprocessor': [preprocessor],\n",
        "     'model__weights': ['uniform', 'distance'],\n",
        "     'model__n_neighbors': ['1','3','5','7','9','11']},    # number of neighbors\n",
        "\n",
        "    {'model': [GradientBoostingClassifier(random_state=0)],\n",
        "     'preprocessor': [preprocessor],\n",
        "     'model__n_estimators': [100, 200, 300, 400, 500],    # number of trees in the forest\n",
        "     'model__max_depth': [1],         # maximun depth of the trees\n",
        "     'model__learning_rate': [0.01, 0.1, 1, 10, 100]},\n",
        "\n",
        "    {'model': [SVC()],\n",
        "     'preprocessor': [preprocessor],\n",
        "     'model__C': [0.1, 1, 10, 100],\n",
        "     'model__kernel': ['linear','rbf', 'poly']}] #types of svc\n",
        "\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(pipe, param_grid, cv=5, return_train_score=True)"
      ],
      "metadata": {
        "id": "dtsYlmyCrCMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "b6dIEvl-vbPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "7Ae-Ym9052LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best params:\\n{}\\n\".format(grid_search.best_params_))\n",
        "print(\"Best cross-validation train score: {:.2f}\".format(grid_search.cv_results_['mean_train_score'][grid_search.best_index_]))\n",
        "print(\"Best cross-validation test score: {:.2f}\".format(grid_search.best_score_))\n",
        "print(\"Test-set score: {:.2f}\".format(grid_search.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "o9ctuTo_4kEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the variables\n",
        "best_params = grid_search.best_params_\n",
        "best_train_score = grid_search.cv_results_['mean_train_score'][grid_search.best_index_]\n",
        "best_cv_score = grid_search.best_score_\n",
        "test_score = grid_search.score(X_test, y_test)\n",
        "\n",
        "# Create the DataFrame\n",
        "best_results = pd.DataFrame({\n",
        "    'Metric': ['Best cross-validation train Score', 'Best cross-Validation test Score', 'Test-set score'],\n",
        "    'Value': [best_train_score, best_cv_score, test_score]\n",
        "})\n",
        "\n",
        "# Display DataFrame\n",
        "best_results"
      ],
      "metadata": {
        "id": "csem88Cx-jWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After comparing four classification models on our dataset; the results suggest that a Gradient Boosting Classifier model with specific hyperparameters achieved impressive performance on both the training and testing datasets:\n",
        "\n",
        "Best Parameters:\n",
        "\n",
        "The best model selected is a GradientBoostingClassifier with the following hyperparameters:\n",
        "max_depth: 1\n",
        "n_estimators: 500\n",
        "learning_rate: 0.1\n",
        "The preprocessing pipeline includes standard scaling for numerical features and one-hot encoding for categorical features.\n",
        "\n",
        "Performance Metrics:\n",
        "\n",
        "Best Cross-Validation Train Score: 0.99\n",
        "This indicates that the model achieved a high level of accuracy (99%) on the training data during cross-validation. It suggests that the model effectively learned the patterns in the training data.\n",
        "Best Cross-Validation Test Score: 0.97\n",
        "The model also performed exceptionally well on unseen data, achieving a cross-validation accuracy of 97%. This suggests that the model has good generalization capability and is not overfitting to the training data.\n",
        "Test-Set Score: 0.98\n",
        "The accuracy of 98% on the test set further validates the model's performance. It indicates that the model maintains high accuracy when applied to completely unseen data, reinforcing confidence in its effectiveness.\n",
        "\n",
        "Insights:\n",
        "\n",
        "The high scores across all metrics suggest that the selected model with the specified hyperparameters and preprocessing steps is well-suited for the classification task.\n",
        "The model demonstrates strong predictive power, achieving near-perfect accuracy on both the training and testing datasets.\n",
        "The preprocessing steps, including standard scaling and one-hot encoding, effectively prepared the data for modeling, contributing to the model's overall performance.\n",
        "\n",
        "Overall, the results indicate that the Gradient Boosting Classifier, with appropriate tuning and preprocessing, can effectively classify the data and make accurate predictions."
      ],
      "metadata": {
        "id": "xrculmNtJw-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#now we do error analysis to conclude using a confusion matrix heatmap\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "validation_score = accuracy_score(y_test_pred, y_test)"
      ],
      "metadata": {
        "id": "2bqxpRB0EIDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "Conf_Mat = confusion_matrix(y_test,y_test_pred )\n",
        "my_labels = ['Rejected','Approved']\n",
        "print(Conf_Mat)"
      ],
      "metadata": {
        "id": "nYeKdg0UEdzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(Conf_Mat, square=True, annot=True, cbar=False, fmt='d', xticklabels=my_labels, yticklabels=my_labels)\n",
        "plt.xlabel('Predicted Value')\n",
        "plt.ylabel('True Value')\n",
        "plt.title('Confusion Matrix')"
      ],
      "metadata": {
        "id": "RcPYgEpFE1bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we conducted an error analysis for our model's predictions and got the following insights;\n",
        "\n",
        "Accuracy is a measure of how often the classifier is correct.\n",
        "\n",
        "It is calculated as (TP + TN) / (TP + TN + FP + FN).\n",
        "In this case, the accuracy can be calculated as (780 + 475) / (780 + 475 + 17 + 9) = 0.979, or 97.9%.\n",
        "\n",
        "The high number of true positives (475) and true negatives (780) indicates that the model is making accurate predictions for both classes.\n",
        "\n",
        "The low number of false positives (17) and false negatives (9) suggests that the model has a low rate of misclassification.\n",
        "\n",
        "The overall accuracy of 97.9% suggests that the model is performing well in terms of correctly classifying instances from the test dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "v0AlyeeNNKUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search.best_params\n"
      ],
      "metadata": {
        "id": "CkiLdcv0Ue5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L9fI-DHmUgM4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}